---
layout: post
title: Exploring MTA Tweets
date: 2018-08-29
---
I recently got my hands on a dataset of all the tweets from MTA's official Twitter account in 2017 and up to July of 2018 from a colleague. He gathered it by scraping from Twitter's API. There wasn't really a target to predict, so this was mostly a practice in exploring data. No massive revelations were discovered, but there were some interesting bits to pull from the process. The repo for this project can be found on Github [here](https://github.com/confoley/mta_tweets). 

After cleaning the tweets up, I decided to incorporate some unsupervised learning techniques and use [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to identify topics in the tweets. After looking at the results with two, three, and four identified topics, clustering them into two made most sense. Essentially the tweets can be categorized into a service update category, using words like _delay, service, indicent, resumed, problem, running, allow, additional, signal, mechanical_, and an apology category, using words/n_grams like _regret, supervision, thank, inconvenience, matter, report, change, thanks_. 


```python
# gensim prepares interavtive LDA model
# two general categories: service updates and apologies/explanations for those updates
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
```





<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css">


<div id="ldavis_el86981126466441449777425293"></div>
<script type="text/javascript">

var ldavis_el86981126466441449777425293_data = {"mdsDat": {"Freq": [55.0794792175293, 44.920528411865234], "cluster": [1, 1], "topics": [1, 2], "x": [0.28840017318725586, -0.28840017318725586], "y": [0.0, 0.0]}, "tinfo": {"Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2"], "Freq": [36745.0, 43027.0, 36421.0, 19767.0, 19447.0, 18831.0, 23610.0, 20035.0, 19975.0, 18877.0, 18447.0, 17730.0, 17659.0, 12747.0, 12999.0, 14889.0, 45156.0, 10387.0, 14858.0, 8951.0, 8716.0, 7875.0, 7874.0, 6999.0, 9258.0, 13956.0, 6434.0, 8490.0, 6765.0, 5813.0, 43025.95703125, 36420.875, 19974.38671875, 18446.8359375, 18876.48046875, 17729.58984375, 14888.1259765625, 20034.25390625, 9257.0849609375, 8489.4189453125, 6456.5810546875, 6714.09619140625, 5238.43505859375, 5031.03125, 5040.4169921875, 5013.5810546875, 4553.8896484375, 4042.547607421875, 4003.343505859375, 3792.3759765625, 4031.990966796875, 4020.567138671875, 3536.224365234375, 3142.51123046875, 3144.06005859375, 3073.541015625, 3195.779052734375, 3153.48779296875, 2842.998046875, 2749.966552734375, 17637.212890625, 23283.384765625, 14709.541015625, 4242.119140625, 3559.479736328125, 13293.3740234375, 39040.015625, 8015.49658203125, 5041.3544921875, 6394.4150390625, 6781.0146484375, 5249.38134765625, 5472.13330078125, 19767.345703125, 18830.54296875, 19446.900390625, 12747.20703125, 10386.7138671875, 8715.75390625, 8950.7548828125, 6998.6015625, 7874.25341796875, 7874.7080078125, 6433.82568359375, 5668.39501953125, 5813.3876953125, 5569.34912109375, 5164.77099609375, 4980.400390625, 4925.99658203125, 4680.357421875, 4891.7861328125, 4372.18310546875, 4343.4970703125, 4683.3525390625, 4242.78955078125, 4116.0654296875, 3927.25634765625, 3695.890625, 3448.224365234375, 3414.322998046875, 3364.85205078125, 3203.10302734375, 36734.03515625, 3575.380859375, 12823.625, 6640.99462890625, 7541.9130859375, 4178.02392578125, 5631.37890625, 4328.052734375, 6115.9970703125, 4724.98193359375], "Term": ["regret", "delay", "servicealert", "supervision", "info", "mta", "time", "incident", "resumed", "problem", "earlier", "running", "travel", "ref", "condition", "allow", "service", "thank", "additional", "report", "com", "bit", "ly", "twitter", "signal", "following", "matter", "direction", "inconvenience", "pic", "delay", "servicealert", "resumed", "earlier", "problem", "running", "allow", "incident", "signal", "direction", "mechanical", "track", "bound", "shortly", "express", "passenger", "local", "42", "board", "sick", "causing", "able", "34", "arrive", "left", "59", "delayed", "activity", "switch", "waiting", "travel", "time", "additional", "yes", "tell", "following", "service", "location", "hi", "good", "station", "morning", "line", "supervision", "mta", "info", "ref", "thank", "com", "report", "twitter", "ly", "bit", "matter", "reference", "pic", "information", "subway", "notify", "notified", "aware", "detail", "thanks", "note", "\u00e4", "car", "review", "www", "forward", "attention", "bringing", "number", "unpleasant", "regret", "provide", "condition", "inconvenience", "change", "update", "line", "work", "service", "station"], "Total": [36745.0, 43027.0, 36421.0, 19767.0, 19447.0, 18831.0, 23610.0, 20035.0, 19975.0, 18877.0, 18447.0, 17730.0, 17659.0, 12747.0, 12999.0, 14889.0, 45156.0, 10387.0, 14858.0, 8951.0, 8716.0, 7875.0, 7874.0, 6999.0, 9258.0, 13956.0, 6434.0, 8490.0, 6765.0, 5813.0, 43027.02734375, 36421.9609375, 19975.322265625, 18447.767578125, 18877.462890625, 17730.576171875, 14889.044921875, 20035.595703125, 9258.052734375, 8490.44140625, 6457.5341796875, 6715.19140625, 5239.36181640625, 5031.97705078125, 5041.39501953125, 5014.65234375, 4554.94921875, 4043.5009765625, 4004.30712890625, 3793.319091796875, 4032.99462890625, 4021.568359375, 3537.166748046875, 3143.439208984375, 3145.010986328125, 3074.509765625, 3196.787353515625, 3154.5087890625, 2843.978515625, 2750.976318359375, 17659.10546875, 23610.455078125, 14858.658203125, 4243.9140625, 3561.010498046875, 13956.1748046875, 45156.01171875, 8654.125, 5990.04150390625, 9993.650390625, 11505.99609375, 8017.6669921875, 11103.51171875, 19767.810546875, 18831.1015625, 19447.4921875, 12747.669921875, 10387.203125, 8716.2333984375, 8951.265625, 6999.095703125, 7874.818359375, 7875.27490234375, 6434.2900390625, 5668.85791015625, 5813.884765625, 5569.8291015625, 5165.2470703125, 4980.865234375, 4926.4619140625, 4680.8232421875, 4892.28515625, 4372.6494140625, 4343.9677734375, 4683.86767578125, 4243.26611328125, 4116.5283203125, 3927.735595703125, 3696.35546875, 3448.69091796875, 3414.7861328125, 3365.32421875, 3203.565185546875, 36745.7890625, 3575.922119140625, 12999.4482421875, 6765.544921875, 10700.6611328125, 4847.75390625, 11103.51171875, 5587.1376953125, 45156.01171875, 11505.99609375], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5964000225067139, 0.5964000225067139, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5963000059127808, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5961999893188477, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960999727249146, 0.5960000157356262, 0.5960000157356262, 0.5952000021934509, 0.5824000239372253, 0.5863000154495239, 0.5960000157356262, 0.5960000157356262, 0.5476999878883362, 0.45089998841285706, 0.5196999907493591, 0.42399999499320984, 0.14990000426769257, 0.06769999861717224, 0.1729000061750412, -0.1111999973654747, 0.8003000020980835, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001999855041504, 0.8001000285148621, 0.8001000285148621, 0.8001000285148621, 0.8001000285148621, 0.8001000285148621, 0.800000011920929, 0.8001000285148621, 0.7867000102996826, 0.7817000150680542, 0.4503999948501587, 0.6516000032424927, 0.12139999866485596, 0.5449000000953674, -1.1988999843597412, -0.08969999849796295], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.853300094604492, -3.0199999809265137, -3.620699882507324, -3.700200080871582, -3.6772000789642334, -3.7399001121520996, -3.914599895477295, -3.6177000999450684, -4.389699935913086, -4.47629976272583, -4.75, -4.710899829864502, -4.959099769592285, -4.999499797821045, -4.997600078582764, -5.002999782562256, -5.099100112915039, -5.218200206756592, -5.228000164031982, -5.282100200653076, -5.220900058746338, -5.223700046539307, -5.352099895477295, -5.470099925994873, -5.469600200653076, -5.492300033569336, -5.4532999992370605, -5.466599941253662, -5.570300102233887, -5.603499889373779, -3.7451000213623047, -3.467400074005127, -3.9265999794006348, -5.170100212097168, -5.3454999923706055, -4.027900218963623, -2.950500011444092, -4.533699989318848, -4.997399806976318, -4.759699821472168, -4.701000213623047, -4.956999778747559, -4.91540002822876, -3.4272000789642334, -3.475800037384033, -3.44350004196167, -3.8659000396728516, -4.070700168609619, -4.246099948883057, -4.2195000648498535, -4.4654998779296875, -4.347599983215332, -4.347599983215332, -4.549699783325195, -4.676300048828125, -4.651100158691406, -4.693999767303467, -4.769400119781494, -4.805699825286865, -4.816699981689453, -4.8678998947143555, -4.823699951171875, -4.935999870300293, -4.942599773406982, -4.867199897766113, -4.966000080108643, -4.996300220489502, -5.043300151824951, -5.104000091552734, -5.173399925231934, -5.183300018310547, -5.19789981842041, -5.247099876403809, -2.807499885559082, -5.137199878692627, -3.8598999977111816, -4.51800012588501, -4.3907999992370605, -4.981400012969971, -4.6828999519348145, -4.946100234985352, -4.600299835205078, -4.858399868011475]}, "token.table": {"Topic": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2], "Freq": [0.999670147895813, 0.0002827121352311224, 0.999876081943512, 0.0002473104395903647, 0.9998341798782349, 0.0003252550959587097, 0.9998586773872375, 0.0002486591984052211, 0.9995217323303223, 0.0003170065756421536, 0.9899951815605164, 0.01002782303839922, 0.9999298453330994, 6.716347706969827e-05, 0.9998602867126465, 0.00031812291126698256, 0.9997996687889099, 0.9998241066932678, 0.00012697969214059412, 0.9999650716781616, 0.9996735453605652, 0.0002497310924809426, 0.9997400641441345, 0.00019086293468717486, 0.9997698068618774, 0.9999372959136963, 0.9997533559799194, 0.0002479547110851854, 0.2952153980731964, 0.7048162817955017, 0.9999732375144958, 0.013539035804569721, 0.9865034222602844, 0.9999760985374451, 2.3241205781232566e-05, 0.9997537136077881, 0.0003128140524495393, 0.9999417066574097, 0.9998302459716797, 0.0001177795056719333, 0.9999583959579468, 5.420709931058809e-05, 0.9997233152389526, 0.0001983577967621386, 0.9524816274642944, 0.047505855560302734, 0.9999038577079773, 0.6398062705993652, 0.3601286709308624, 0.8415634632110596, 0.15842962265014648, 0.9999203681945801, 4.9911170208361e-05, 0.0184759683907032, 0.9815912842750549, 5.1420513045741245e-05, 0.9999746680259705, 0.9998511672019958, 0.9996785521507263, 0.0003179639170411974, 0.49281707406044006, 0.507136881351471, 0.9997916221618652, 0.0002195414126617834, 0.9261479377746582, 0.07383762300014496, 0.00012698705540969968, 0.9998960494995117, 0.9999549388885498, 0.9999172687530518, 0.0001548578729853034, 0.6546792387962341, 0.3452375829219818, 5.3103638492757455e-05, 0.9999946355819702, 0.9997771978378296, 0.9999062418937683, 0.9998263120651245, 0.999903678894043, 0.9998698830604553, 0.00019941561913583428, 0.9998478293418884, 0.999922513961792, 5.2973220590502024e-05, 0.00027964814216829836, 0.9997421503067017, 0.9999474287033081, 0.9998486638069153, 0.0003265680279582739, 0.9996791481971741, 0.00011171604273840785, 0.9999703168869019, 0.9999337792396545, 5.0061771617038175e-05, 0.9998716711997986, 0.9999675154685974, 5.639974551741034e-05, 0.864558219909668, 0.13544154167175293, 0.9999735951423645, 2.7455962481326424e-05, 0.9998058080673218, 0.0001987290452234447, 0.9996522665023804, 0.00026362139033153653, 0.9998862743377686, 0.0001080140718840994, 0.5893449187278748, 0.4106554388999939, 0.9999521374702454, 0.9999589920043945, 0.9996559619903564, 0.0003516201104503125, 0.9994354248046875, 0.000561638327781111, 0.9999804496765137, 0.9998514652252197, 0.986130952835083, 0.01384979672729969, 0.9998225569725037, 0.00014891607861500233, 0.9987481832504272, 0.0012458162382245064, 0.9999863505363464, 0.9998235702514648, 0.1382083296775818, 0.8618424534797668, 0.9996451139450073, 0.00036350730806589127, 0.22533899545669556, 0.7746363878250122, 0.9998127222061157, 0.9995489716529846, 0.00047126307617872953, 0.00021349877351894975, 0.9998147487640381], "Term": ["34", "34", "42", "42", "59", "59", "able", "able", "activity", "activity", "additional", "additional", "allow", "allow", "arrive", "arrive", "attention", "aware", "bit", "bit", "board", "board", "bound", "bound", "bringing", "car", "causing", "causing", "change", "change", "com", "condition", "condition", "delay", "delay", "delayed", "delayed", "detail", "direction", "direction", "earlier", "earlier", "express", "express", "following", "following", "forward", "good", "good", "hi", "hi", "incident", "incident", "inconvenience", "inconvenience", "info", "info", "information", "left", "left", "line", "line", "local", "local", "location", "location", "ly", "ly", "matter", "mechanical", "mechanical", "morning", "morning", "mta", "mta", "note", "notified", "notify", "number", "passenger", "passenger", "pic", "problem", "problem", "provide", "provide", "ref", "reference", "regret", "regret", "report", "report", "resumed", "resumed", "review", "running", "running", "service", "service", "servicealert", "servicealert", "shortly", "shortly", "sick", "sick", "signal", "signal", "station", "station", "subway", "supervision", "switch", "switch", "tell", "tell", "thank", "thanks", "time", "time", "track", "track", "travel", "travel", "twitter", "unpleasant", "update", "update", "waiting", "waiting", "work", "work", "www", "yes", "yes", "\u00e4", "\u00e4"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [2, 1]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el86981126466441449777425293", ldavis_el86981126466441449777425293_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
        new LDAvis("#" + "ldavis_el86981126466441449777425293", ldavis_el86981126466441449777425293_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js", function(){
         LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el86981126466441449777425293", ldavis_el86981126466441449777425293_data);
            })
         });
}
</script>



Next we decided to create a target to predict. Using Regex, we identified the MTA employee who was the author of each tweet, which was signified by a circumflex and the authors initials. We then used some NLP tools to use the tweet (with the author's signature removed) to predict who the author was. The high accuracy rate of 84% on test data with a simple multiclass Logistic Regression model was surprising, and very interesting that it was able to pick up on the subtle differences in language between authors. There's not whole lot of value in these findings, but it's a good practice and interesting to see how these models work. 


```python
tweeters[:10]
# Top ten authors we were predicting
```




    ['^JG', '^JP', '^BD', '^KF', '^GES', '^DG', '^JZ', '^HKD', '^RT', '^EE']




```python
y.value_counts()
# Baseline accuracy is 20.28%
```




    ^JG     15382
    ^JP     13759
    ^BD     11511
    ^KF      6821
    ^GES     5762
    ^DG      5206
    ^JZ      4714
    ^HKD     4527
    ^RT      4442
    ^EE      3693
    Name: sig, dtype: int64



These were features with the highest feature importances from a Random Forest Classifier model. This model worked with 80% accuracy on test data.


```python
top_feat_importances = pd.DataFrame(list(zip(rf.feature_importances_, cv.get_feature_names())),
             columns=['f_importance','feature']).sort_values('f_importance', ascending=False).head(20)
top_feat_importances
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>f_importance</th>
      <th>feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>25933</th>
      <td>0.006591</td>
      <td>good evening</td>
    </tr>
    <tr>
      <th>64959</th>
      <td>0.006346</td>
      <td>time</td>
    </tr>
    <tr>
      <th>61677</th>
      <td>0.005757</td>
      <td>supervision</td>
    </tr>
    <tr>
      <th>63703</th>
      <td>0.005720</td>
      <td>thank</td>
    </tr>
    <tr>
      <th>28848</th>
      <td>0.005119</td>
      <td>hi</td>
    </tr>
    <tr>
      <th>37829</th>
      <td>0.005099</td>
      <td>location</td>
    </tr>
    <tr>
      <th>63199</th>
      <td>0.004789</td>
      <td>tell</td>
    </tr>
    <tr>
      <th>50852</th>
      <td>0.004534</td>
      <td>regrets</td>
    </tr>
    <tr>
      <th>40539</th>
      <td>0.004356</td>
      <td>morning</td>
    </tr>
    <tr>
      <th>50183</th>
      <td>0.004141</td>
      <td>reference</td>
    </tr>
    <tr>
      <th>25472</th>
      <td>0.004011</td>
      <td>good</td>
    </tr>
    <tr>
      <th>59799</th>
      <td>0.003989</td>
      <td>station</td>
    </tr>
    <tr>
      <th>21404</th>
      <td>0.003985</td>
      <td>en</td>
    </tr>
    <tr>
      <th>48461</th>
      <td>0.003833</td>
      <td>proceeding</td>
    </tr>
    <tr>
      <th>50472</th>
      <td>0.003588</td>
      <td>referring</td>
    </tr>
    <tr>
      <th>26551</th>
      <td>0.003408</td>
      <td>good morning</td>
    </tr>
    <tr>
      <th>64733</th>
      <td>0.003396</td>
      <td>thanks</td>
    </tr>
    <tr>
      <th>10138</th>
      <td>0.003297</td>
      <td>bound</td>
    </tr>
    <tr>
      <th>42163</th>
      <td>0.003222</td>
      <td>mta nyc custhelp com</td>
    </tr>
    <tr>
      <th>49950</th>
      <td>0.003149</td>
      <td>ref</td>
    </tr>
  </tbody>
</table>
</div>



Looking at the highest value in the list of probabilities for each target, we can get a sense of how sure the model is of its predictions. You can see that the Logistic Regression model, which was also more accurate, has a distribution of probabilities that indicates it is more confident in its predictions than the Random Forest model.


```python
# How sure is the model about its predictions?
plt.figure(figsize=(12,7))
plt.title("Distribution of Highest Probability Values, Random Forest Model", fontsize=18)
plt.xlabel("Probability")
plt.ylabel("Number of Tweets")
plt.hist(max_probs, color='navy', bins=20);
```


![png](/images/mta_blog_files/mta_blog_9_0.png)



```python
# Most probabilities are much higher, model is more sure of its predictions
plt.figure(figsize=(12,7))
plt.title("Distribution of Highest Probability Values, LogReg Model", fontsize=18)
plt.xlabel("Probability")
plt.ylabel("Number of Tweets")
plt.hist(max_lr_probs, color='navy', bins=30);
```


![png](/images/mta_blog_files/mta_blog_10_0.png)


To find which features were most important overall, I created a dataframe of each coefficient by author (from the Logistic Regression model), with the sum and mean of the absolute values of the coefficients. The following plots show which features were most important overall and by author.


```python
abs_coef_df = abs(coef_df)
abs_coef_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>jg</th>
      <th>jp</th>
      <th>bd</th>
      <th>kf</th>
      <th>ges</th>
      <th>dg</th>
      <th>jz</th>
      <th>hkd</th>
      <th>rt</th>
      <th>ee</th>
      <th>abs_sum</th>
      <th>abs_mean</th>
    </tr>
    <tr>
      <th>feature</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>00</th>
      <td>0.172850</td>
      <td>0.287960</td>
      <td>0.084425</td>
      <td>0.170298</td>
      <td>0.156555</td>
      <td>0.305762</td>
      <td>0.049209</td>
      <td>0.008900</td>
      <td>0.729685</td>
      <td>0.101591</td>
      <td>2.067236</td>
      <td>0.375861</td>
    </tr>
    <tr>
      <th>00 info</th>
      <td>0.119540</td>
      <td>0.000602</td>
      <td>0.005627</td>
      <td>0.000786</td>
      <td>0.035574</td>
      <td>0.005080</td>
      <td>0.003147</td>
      <td>0.000155</td>
      <td>0.172403</td>
      <td>0.001891</td>
      <td>0.344806</td>
      <td>0.062692</td>
    </tr>
    <tr>
      <th>00 info web</th>
      <td>0.119540</td>
      <td>0.000602</td>
      <td>0.005627</td>
      <td>0.000786</td>
      <td>0.035574</td>
      <td>0.005080</td>
      <td>0.003147</td>
      <td>0.000155</td>
      <td>0.172403</td>
      <td>0.001891</td>
      <td>0.344806</td>
      <td>0.062692</td>
    </tr>
    <tr>
      <th>00 info web mta</th>
      <td>0.119540</td>
      <td>0.000602</td>
      <td>0.005627</td>
      <td>0.000786</td>
      <td>0.035574</td>
      <td>0.005080</td>
      <td>0.003147</td>
      <td>0.000155</td>
      <td>0.172403</td>
      <td>0.001891</td>
      <td>0.344806</td>
      <td>0.062692</td>
    </tr>
    <tr>
      <th>00 pm</th>
      <td>0.035549</td>
      <td>0.198963</td>
      <td>0.054368</td>
      <td>0.495864</td>
      <td>0.029549</td>
      <td>0.461121</td>
      <td>0.302342</td>
      <td>0.006940</td>
      <td>0.004700</td>
      <td>0.030295</td>
      <td>1.619691</td>
      <td>0.294489</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Exploring which features were most important across all authors

plt.figure(figsize=(12,8))
plt.title("Features with Largest Coefficient Abs Sums")
abs_coef_df['abs_mean'].sort_values().tail(20).plot(kind='bar')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a5861f898>




![png](/images/mta_blog_files/mta_blog_13_1.png)



```python
# what the correlations are between top features and each author with the abs_mean shown for indication of overall importance
# for example, JP doesn't use the word 'apologies' very often...
coef_df[[col for col in coef_df.columns if col !='abs_sum']].sort_values('abs_mean', ascending=False).head(8).plot\
(kind='barh',figsize=(18,14))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a5a4fd978>




![png](/images/mta_blog_files/mta_blog_14_1.png)

